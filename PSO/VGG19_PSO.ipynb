{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batchsize  = 32\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)               # no data augmentation performed, only scaling from 0 to 1\n",
    "\n",
    "train_generator = test_datagen.flow_from_directory(\n",
    "         r'/notebooks/Kaggle_FinalData_Preprocessed_V3/train',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "         r'/notebooks/Kaggle_FinalData_Preprocessed_V3/validation',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "train_img, labels = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def plotImage(images_arr):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(16,8))\n",
    "    axes = axes.flatten()\n",
    "    for img, img_y, ax in zip(images_arr, labels, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title('Severity {}'.format(np.argmax(img_y, -1)))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "plotImage(train_img)\n",
    "#print(labels)  # uncomment this to check if the severity is correct.\n",
    "print(train_img.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications import vgg19\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "vgg = vgg19.VGG19(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape)\n",
    "num_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vgg.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg.layers:\n",
    "    if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_generator)\n",
    "\n",
    "in_lay = Input(t_x.shape[1:])\n",
    "\n",
    "pt_features = vgg(in_lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "dropout1 = Dropout(0.6)(pt_features)\n",
    "x = Flatten()(dropout1)\n",
    "dense1 = Dense(512, activation='relu')(x)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "prediction = Dense(num_class, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=in_lay, outputs=prediction)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built In Tensorflow Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import CohenKappa\n",
    "from keras.metrics import AUC, TopKCategoricalAccuracy, SpecificityAtSensitivity, Precision, Recall\n",
    "cohen = tfa.metrics.CohenKappa(num_classes=5)\n",
    "cohen_linear = tfa.metrics.CohenKappa(num_classes=5, weightage='linear')\n",
    "cohen_quad = tfa.metrics.CohenKappa(num_classes=5, weightage='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "epochs = 50\n",
    "lrate = 0.001\n",
    "opt = SGD(lr=lrate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy','categorical_accuracy',f1_m,recall_m,precision_m,\n",
    "                       SpecificityAtSensitivity(0.95), AUC(),cohen_quad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_Weights.hdf5\".format('VGG19_PSO_V1_test')\n",
    "filepath = r'Saved Weights/VGG19/weight_path'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=15, verbose=1, mode='auto')\n",
    "# Reducing the Learning Rate if result is not improving. \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6, mode='auto',\n",
    "                              verbose=1)\n",
    "callbacks_list = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "\n",
    "def cnn_for_pso(opt):\n",
    "    input_shape = (224, 224, 3)\n",
    "    vgg = vgg19.VGG19(include_top=False, weights='imagenet', \n",
    "                                         input_shape=input_shape)\n",
    "    num_class = 5\n",
    "\n",
    "    import pandas as pd\n",
    "    vgg.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in vgg.layers:\n",
    "        if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in vgg.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])  \n",
    "\n",
    "    t_x, t_y = next(train_generator)\n",
    "\n",
    "    in_lay = Input(t_x.shape[1:])\n",
    "\n",
    "\n",
    "    pt_features = vgg(in_lay)\n",
    "\n",
    "    x = Flatten()(pt_features)\n",
    "    dense1 = Dense(opt[\"Dense.0\"], activation='relu')(x)\n",
    "    dense2 = Dense(opt[\"Dense.1\"], activation='relu')(dense1)\n",
    "    prediction = Dense(num_class, activation='softmax')(dense2)\n",
    "    \n",
    "    lr = opt[\"LearningRate.0\"]\n",
    "    epochs = 50\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr), metrics=['accuracy','categorical_accuracy',f1_m,recall_m,precision_m,\n",
    "                           SpecificityAtSensitivity(0.95), AUC(),cohen_quad])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=opt[\"StoppingPatience.0\"], verbose=1, mode='auto')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6, mode='auto',\n",
    "                                  verbose=1)\n",
    "    callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "    history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.samples // batchsize,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.samples // batchsize,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks = callbacks_list\n",
    "                    )\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle('VGG19_PSO_V1_test', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    ax1.plot(acc, label='Train Accuracy')\n",
    "    ax1.plot(val_acc, label='Validation Accuracy')\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(loss, label='Train Loss')\n",
    "    ax2.plot(val_loss, label='Validation Loss')\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "    f.savefig(r'/notebooks/Saved Plots/Accuracy_Loss/VGG19/VGG19_PSO_V1_test')\n",
    "    \n",
    "    \n",
    "    model.save('Saved Models/VGG19_PSO_V1_test.h5')   \n",
    "    print('Saved Model to disk')\n",
    "    \n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    img_height = 224\n",
    "    img_width = 224\n",
    "    eval_data_dir = r'/notebooks/Pool2_Preprocessed_V2/test'\n",
    "    eval_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "    eval_generator = eval_datagen.flow_from_directory(\n",
    "        eval_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True) \n",
    "\n",
    "    filenames = eval_generator.filenames\n",
    "    nb_samples = len(filenames)\n",
    "    \n",
    "    predictions = model.predict(eval_generator,nb_samples)\n",
    "    \n",
    "    num_classes = 5\n",
    "    import pandas as pd\n",
    "    truth_array = tf.argmax(pd.get_dummies(pd.Series(eval_generator.classes)),axis=1)\n",
    "    predictions_array = tf.argmax(predictions,axis=1)\n",
    "    print(truth_array)\n",
    "    print(predictions_array)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(truth_array, predictions_array)))\n",
    "    print(classification_report(truth_array,predictions_array))\n",
    "\n",
    "    file1 = open(r'/notebooks/Saved Plots/Accuracy_TestData/VGG19/VGG19_PSO_V1_test.txt',\"w\")\n",
    "    file1.write(str((accuracy_score(truth_array, predictions_array))))\n",
    "    file1.write(\"\\n\")\n",
    "    file1.write(str(classification_report(truth_array,predictions_array)))\n",
    "    file1.close()\n",
    "    \n",
    "    print(max(history.history['val_cohen_kappa']))\n",
    "    \n",
    "    #compute conf mat\n",
    "    cf_matrix = tf.math.confusion_matrix(truth_array,predictions_array)\n",
    "    print(cf_matrix)\n",
    "\n",
    "    import seaborn as sns\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "                fmt='.2%', cmap='Blues')\n",
    "    fig.savefig(r'/notebooks/Saved Plots/Confusion_Matrix/VGG19/VGG_PSO_V1')\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "    sick_vec = truth_array>0\n",
    "    sick_score = np.sum(predictions[:,1:],1)\n",
    "    fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n",
    "    fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n",
    "    ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n",
    "    ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "    fig.savefig(r'/notebooks/Saved Plots/AUC_ROC/VGG19/VGG19_PSO_V1_test')\n",
    "    \n",
    "    \n",
    "    return (1 - min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hyperactive import Hyperactive, ParticleSwarmOptimizer, RandomSearchOptimizer\n",
    "import hyperactive\n",
    "search_space = {\n",
    "    #\"Dropout.0\": list((0.2, 0.4, 0.6, 0.8)),\n",
    "    #\"Dense.0\": list((32, 64, 128, 256, 512,1024)),\n",
    "    #\"Dense.1\": list((32, 64, 128, 256, 512,1024)),\n",
    "    #\"LearningRate.0\": list((0.00001, 0.0001, 0.001, 0.01, 0.1)),\n",
    "    #\"StoppingPatience.0\": list((5,10,15,20,25))\n",
    "    \n",
    "    \n",
    "    \"Dense.0\": list((128, 128)),\n",
    "    \"Dense.1\": list((128, 128)),\n",
    "    \"LearningRate.0\": list((0.001, 0.001)),\n",
    "    \"StoppingPatience.0\": list((15, 15))\n",
    "}\n",
    "\n",
    "optimizer123 = ParticleSwarmOptimizer(\n",
    "    inertia=0.4,\n",
    "    cognitive_weight=0.7,\n",
    "    social_weight=0.7,\n",
    "    temp_weight=0.3,\n",
    "    rand_rest_p=0.05,\n",
    ")\n",
    "\n",
    "hyper = Hyperactive()\n",
    "hyper.add_search(cnn_for_pso, search_space, optimizer=optimizer123, n_iter=1)\n",
    "hyper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
