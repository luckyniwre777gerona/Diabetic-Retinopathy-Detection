{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batchsize  = 32\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)               # no data augmentation performed, only scaling from 0 to 1\n",
    "\n",
    "train_generator = test_datagen.flow_from_directory(\n",
    "        r'/notebooks/Kaggle_FinalData_Preprocessed_V3/train',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        r'/notebooks/Kaggle_FinalData_Preprocessed_V3/validation',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "train_img, labels = next(train_generator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def plotImage(images_arr):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(16,8))\n",
    "    axes = axes.flatten()\n",
    "    for img, img_y, ax in zip(images_arr, labels, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title('Severity {}'.format(np.argmax(img_y, -1)))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "plotImage(train_img)\n",
    "#print(labels)  # uncomment this to check if the severity is correct.\n",
    "print(train_img.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ResNet152 Model with Dropouts and more Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications import resnet\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "resnet = resnet.ResNet152(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape) \n",
    "num_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "resnet.trainable = True\n",
    "set_trainable = False\n",
    "for layer in resnet.layers:\n",
    "    #if layer.name in ['conv4_block1_1_conv']:\n",
    "    if layer.name in ['conv5_block2_1_conv']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "\n",
    "for layer in resnet.layers:\n",
    "    if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "        layer.trainable = False\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_generator)\n",
    "\n",
    "in_lay = Input(t_x.shape[1:])\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(resnet)\n",
    "add_model.add(Dense(2048,activation='relu'))\n",
    "\n",
    "\n",
    "pt_features = resnet(in_lay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "dense_pre = Dense(128, activation='relu')(pt_features)\n",
    "TSSL = Dropout(0.4)(dense_pre)\n",
    "x = Flatten()(TSSL)\n",
    "dense1 = Dense(128, activation='relu')(x)\n",
    "dropout1 = Dropout(0.2)(dense1)\n",
    "prediction = Dense(num_class, activation='softmax')(dropout1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model = Model(inputs=in_lay, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more Performance Metrics before compiling\n",
    "### Source: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built In Performance Metrics on Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import CohenKappa\n",
    "from keras.metrics import AUC, TopKCategoricalAccuracy, SpecificityAtSensitivity, Precision, Recall\n",
    "cohen = tfa.metrics.CohenKappa(num_classes=5)\n",
    "cohen_linear = tfa.metrics.CohenKappa(num_classes=5, weightage='linear')\n",
    "cohen_quad = tfa.metrics.CohenKappa(num_classes=5, weightage='quadratic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "epochs = 50\n",
    "lrate = 0.001\n",
    "opt = SGD(lr=lrate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy','categorical_accuracy',f1_m,recall_m,precision_m,\n",
    "                       SpecificityAtSensitivity(0.95), AUC(),cohen_quad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights_v2.best.hdf5\".format('ResNet152_TSSL_Hyperactive_BestV1_val')\n",
    "filepath = r'/notebooks/Saved Weights/ResNet152/weight_path'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6, mode='auto',\n",
    "                              verbose=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=15, verbose=1, mode='auto') # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.samples // batchsize,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.samples // batchsize,\n",
    "                    verbose=1,\n",
    "                    callbacks = callbacks_list\n",
    "                    )           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('ResNet152_TSSL_Hyperactive_BestV1_val', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1.plot(acc, label='Train Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(loss, label='Train Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "f.savefig(r'/notebooks/Saved Plots/Accuracy_Loss/ResNet152/ResNet152_TSSL_Hyperactive_BestV1_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model.save(r'/notebooks/Saved Models/ResNet152/ResNet152_TSSL_Hyperactive_BestV1_val.h5/')   \n",
    "print('Saved Model to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "def preprocessor_func(img):\n",
    "    IMG_SIZE = 224\n",
    "    sigmaX = 10\n",
    "    b = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    c = crop_image_from_gray(b)\n",
    "    d = cv2.resize(c, (IMG_SIZE, IMG_SIZE))\n",
    "    new_img = cv2.addWeighted ( d,4, cv2.GaussianBlur( d , (0,0) , sigmaX) ,-4 ,128)\n",
    "    return new_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batchsize = 32\n",
    "eval_data_dir = r'/notebooks/Pool2_Preprocessed_V2/test'\n",
    "eval_datagen = ImageDataGenerator(rescale=1./255) # set validation split\n",
    "\n",
    "eval_generator = eval_datagen.flow_from_directory(\n",
    "    eval_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batchsize,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False) \n",
    "\n",
    "filenames = eval_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "eval_img, labels = next(eval_generator)\n",
    "plotImage(eval_img)\n",
    "#print(labels)  # uncomment this to check if the severity is correct.\n",
    "print(eval_img.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = model.predict(eval_generator,nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "import pandas as pd\n",
    "truth_array = tf.argmax(pd.get_dummies(pd.Series(eval_generator.classes)),axis=1)\n",
    "predictions_array = tf.argmax(predictions,axis=1)\n",
    "print(truth_array)\n",
    "print(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(truth_array, predictions_array)))\n",
    "print(classification_report(truth_array,predictions_array))\n",
    "\n",
    "file1 = open(r'/notebooks/Saved Plots/Accuracy_TestData/ResNet152/ResNet152_TSSL_Hyperactive_BestV1_val.txt',\"w\")\n",
    "file1.write(str((accuracy_score(truth_array, predictions_array))))\n",
    "file1.write(\"\\n\")\n",
    "file1.write(str(classification_report(truth_array,predictions_array)))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "print(max(history.history['val_cohen_kappa']))\n",
    "\n",
    "print(max(history.history['val_specificity_at_sensitivity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "#compute conf mat\n",
    "cf_matrix = tf.math.confusion_matrix(truth_array,predictions_array)\n",
    "print(cf_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "fig.savefig(r'/notebooks/Saved Plots/Confusion_Matrix/ResNet152/ResNet152_TSSL_Hyperactive_BestV1_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "sick_vec = truth_array>0\n",
    "sick_score = np.sum(predictions[:,1:],1)\n",
    "fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n",
    "ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n",
    "ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "fig.savefig(r'/notebooks/Saved Plots/AUC_ROC/ResNet152/ResNet152_TSSL_Hyperactive_BestV1_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(truth_array, predictions_array, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
