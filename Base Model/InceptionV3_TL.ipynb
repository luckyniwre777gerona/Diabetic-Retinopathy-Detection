{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batchsize  = 32\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)               # no data augmentation performed, only scaling from 0 to 1\n",
    "\n",
    "train_generator = test_datagen.flow_from_directory(\n",
    "        r'/notebooks/Kaggle_FinalData_Preprocessed_V3/train',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        r'/notebooks/Kaggle_FinalData_Preprocessed_V3/validation',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batchsize,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, labels = next(train_generator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImage(images_arr):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(16,8))\n",
    "    axes = axes.flatten()\n",
    "    for img, img_y, ax in zip(images_arr, labels, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title('Severity {}'.format(np.argmax(img_y, -1)))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImage(train_img)\n",
    "#print(labels)  # uncomment this to check if the severity is correct.\n",
    "print(train_img.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the InceptionV3 Model with Dropouts and more Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "inception = InceptionV3(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape) \n",
    "num_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in inception.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in inception.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_generator)\n",
    "\n",
    "in_lay = Input(t_x.shape[1:])\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(inception)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "add_model.add(Dropout(0.1))\n",
    "\n",
    "add_model.add(Flatten())\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dropout(0.5))\n",
    "add_model.add(Dense(32, activation='relu'))\n",
    "add_model.add(Dense(num_class, activation='softmax', name = \"prediction\"))\n",
    "model=add_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more Performance Metrics before compiling\n",
    "### Source: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built In Performance Metrics on Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import CohenKappa\n",
    "from keras.metrics import AUC, TopKCategoricalAccuracy, SpecificityAtSensitivity, Precision, Recall\n",
    "cohen = tfa.metrics.CohenKappa(num_classes=5)\n",
    "cohen_linear = tfa.metrics.CohenKappa(num_classes=5, weightage='linear')\n",
    "cohen_quad = tfa.metrics.CohenKappa(num_classes=5, weightage='quadratic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=optimizers.SGD(lr=0.1),\n",
    "  metrics=['accuracy','categorical_accuracy',f1_m,recall_m,precision_m,\n",
    "                       SpecificityAtSensitivity(0.95), AUC(),cohen_quad]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights_v2.best.hdf5\".format('InceptionV3_TL_Hyperactive_Best_V1_test')\n",
    "filepath = r'/notebooks/Saved Weights/InceptionV3/weight_path'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min', min_delta=0.01, cooldown=5, min_lr=0.00001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\",\n",
    "                      min_delta=0.03,\n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.samples // batchsize,\n",
    "                    epochs=50,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.samples // batchsize,\n",
    "                    verbose=1,\n",
    "                    callbacks = callbacks_list\n",
    "                    )           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('InceptionV3_TL_Hyperactive_Best_V1_test', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1.plot(acc, label='Train Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(loss, label='Train Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "f.savefig(r'/notebooks/Saved Plots/Accuracy_Loss/InceptionV3/InceptionV3_TL_Hyperactive_Best_V1_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'/notebooks/Saved Models/InceptionV3/InceptionV3_TL_Hyperactive_Best_V1_test.h5/')   \n",
    "print('Saved Model to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "def preprocessor_func(img):\n",
    "    IMG_SIZE = 224\n",
    "    sigmaX = 10\n",
    "    b = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    c = crop_image_from_gray(b)\n",
    "    d = cv2.resize(c, (IMG_SIZE, IMG_SIZE))\n",
    "    new_img = cv2.addWeighted ( d,4, cv2.GaussianBlur( d , (0,0) , sigmaX) ,-4 ,128)\n",
    "    return new_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batchsize = 32\n",
    "eval_data_dir = r'/notebooks/Pool2_Preprocessed_V2/test'\n",
    "eval_datagen = ImageDataGenerator(rescale=1./255) # set validation split\n",
    "\n",
    "eval_generator = eval_datagen.flow_from_directory(\n",
    "    eval_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batchsize,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False) \n",
    "\n",
    "filenames = eval_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_img, labels = next(eval_generator)\n",
    "plotImage(eval_img)\n",
    "#print(labels)  # uncomment this to check if the severity is correct.\n",
    "print(eval_img.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = model.predict(eval_generator,nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "import pandas as pd\n",
    "truth_array = tf.argmax(pd.get_dummies(pd.Series(eval_generator.classes)),axis=1)\n",
    "predictions_array = tf.argmax(predictions,axis=1)\n",
    "print(truth_array)\n",
    "print(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(truth_array, predictions_array)))\n",
    "print(classification_report(truth_array,predictions_array))\n",
    "\n",
    "file1 = open(r'/notebooks/Saved Plots/Accuracy_TestData/InceptionV3/InceptionV3_TL_Hyperactive_Best_V1_test.txt',\"w\")\n",
    "file1.write(str((accuracy_score(truth_array, predictions_array))))\n",
    "file1.write(\"\\n\")\n",
    "file1.write(str(classification_report(truth_array,predictions_array)))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(history.history['val_cohen_kappa']))\n",
    "\n",
    "print(max(history.history['val_specificity_at_sensitivity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute conf mat\n",
    "cf_matrix = tf.math.confusion_matrix(truth_array,predictions_array)\n",
    "print(cf_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "fig.savefig(r'/notebooks/Saved Plots/Confusion_Matrix/InceptionV3/InceptionV3_TL_Hyperactive_Best_V1_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "sick_vec = truth_array>0\n",
    "sick_score = np.sum(predictions[:,1:],1)\n",
    "fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n",
    "ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n",
    "ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "fig.savefig(r'/notebooks/Saved Plots/AUC_ROC/InceptionV3/InceptionV3_TL_Hyperactive_Best_V1_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(truth_array, predictions_array, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
